{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a56ade-29ec-48c4-baaa-c9647d97fba4",
   "metadata": {},
   "source": [
    "# An Adaptive Hybrid Clustering Framework with Iterative Noise Filtering and a Novel DBNS Ratio Validity Measure for Outlier Resilient High-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a60d72-62e4-4aa7-a5a7-04afa46604c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\psbis\\Anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading dataset: anemia.csv\n",
      "Dataset shape: (1421, 5), Number of clusters: 2\n",
      "WARNING:tensorflow:From C:\\Users\\psbis\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Autoencoder compiled with 2D latent space\n",
      "\n",
      "============================================================\n",
      "STARTING HYBRID CLUSTERING WITH DBNS\n",
      "============================================================\n",
      "Training autoencoder...\n",
      "WARNING:tensorflow:From C:\\Users\\psbis\\Anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\psbis\\Anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Computing initial clustering metrics...\n",
      "[Refine 1] tau=1.895, DBNS=0.523, ARI=0.060, F1=0.371, HybridLoss=42.953\n",
      "[Refine 2] tau=1.895, DBNS=0.523, ARI=0.060, F1=0.371, HybridLoss=42.953\n",
      "[Refine 3] tau=1.895, DBNS=0.523, ARI=0.060, F1=0.371, HybridLoss=42.953\n",
      "Running τ sensitivity analysis...\n",
      "Running θ sensitivity analysis...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hybrid Clustering with DBNS (Silhouette-to-Davies-Bouldin Ratio)\n",
    "================================================================\n",
    "\n",
    "A comprehensive clustering framework that combines autoencoder-based dimensionality\n",
    "reduction with a novel hybrid loss function incorporating the DBNS ratio for\n",
    "improved clustering performance.\n",
    "\n",
    "Key Features:\n",
    "- Autoencoder-based 2D latent space representation\n",
    "- Mahalanobis distance-based outlier detection with adaptive re-weighting\n",
    "- DBNS-optimized hybrid loss function (Reconstruction + Outlier penalty - DBNS)\n",
    "- Comprehensive sensitivity analysis for τ, θ, and λ parameters\n",
    "- Ablation studies to evaluate component contributions\n",
    "- Extensive visualization suite including t-SNE plots and learning curves\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ====== Imports ======\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, adjusted_rand_score, f1_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# ====== Configuration ======\n",
    "# Dataset selection - choose one by uncommenting the appropriate line\n",
    "file_path = \"anemia.csv\"     # <--- Update as needed\n",
    "#file_path = \"glass.csv\"       # Glass identification dataset\n",
    "#file_path = \"heart.csv\"      # Heart disease dataset\n",
    "#file_path = \"hepatitis.csv\"  # Hepatitis medical dataset\n",
    "#file_path = \"ionosphere.csv\" # Ionosphere radar data\n",
    "#file_path = \"lymphography.csv\" # Lymphography medical data\n",
    "#file_path = \"parkinsons.csv\" # Parkinson's disease data\n",
    "#file_path = \"pima.csv\"       # Pima Indians diabetes dataset\n",
    "#file_path = \"syn1.csv\"       # Synthetic dataset 1\n",
    "#file_path = \"syn2.csv\"       # Synthetic dataset 2\n",
    "\n",
    "# Algorithm parameters\n",
    "enable_outlier_detection = True  # Enable/disable outlier detection module\n",
    "initial_tau = 1.5                # Initial outlier threshold (Mahalanobis distance)\n",
    "theta = 0.5                      # Reweighting factor for outlier samples\n",
    "val_split = 0.20                 # Validation split ratio for autoencoder training\n",
    "batch_size = 8                   # Batch size for training\n",
    "max_epochs = 50                  # Maximum training epochs\n",
    "max_iters = 3                    # Maximum DBNS refinement iterations\n",
    "base_plots_dir = \"Results Hybrid Clustering with DBNS\"  # Base directory for results\n",
    "\n",
    "# ====== Data Loading & Preprocessing ======\n",
    "dataset_name = os.path.basename(file_path)\n",
    "dataset_base = os.path.splitext(dataset_name)[0]\n",
    "\n",
    "# Create dataset-specific directory structure for organized results\n",
    "plots_dir = os.path.join(base_plots_dir, dataset_base)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# --- Create organized subfolders for different types of results ---\n",
    "subdirs = {\n",
    "    \"learning\": os.path.join(plots_dir, \"learning_curves\"),      # Training curves\n",
    "    \"metrics\": os.path.join(plots_dir, \"cluster_metrics\"),       # Clustering metrics\n",
    "    \"tsne\": os.path.join(plots_dir, \"tsne\"),                     # t-SNE visualizations\n",
    "    \"outlier\": os.path.join(plots_dir, \"outlier\"),               # Outlier analysis\n",
    "    \"csv\": os.path.join(plots_dir, \"csv\"),                       # CSV data files\n",
    "    \"pdf\": os.path.join(plots_dir, \"pdf\"),                       # PDF reports\n",
    "    \"sensitivity\": os.path.join(plots_dir, \"sensitivity\"),       # Parameter sensitivity\n",
    "    \"ablation\": os.path.join(plots_dir, \"ablation\")              # Ablation studies\n",
    "}\n",
    "\n",
    "# Create all subdirectories\n",
    "for p in subdirs.values():\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# ====== Runtime timer ======\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and preprocess data\n",
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "data = pd.read_csv(file_path)\n",
    "X = data.iloc[:, :-1].copy()  # Features (all columns except last)\n",
    "y_true = data.iloc[:, -1].copy()  # Ground truth labels (last column)\n",
    "\n",
    "# Standardize features for better autoencoder performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "true_k = len(np.unique(y_true))  # Number of true clusters\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}, Number of clusters: {true_k}\")\n",
    "\n",
    "# ====== Autoencoder Definition ======\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 2  # 2D latent space for visualization\n",
    "\n",
    "# Autoencoder architecture\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\", name=\"latent\")(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
    "\n",
    "autoencoder = Model(input_layer, decoder, name=\"autoencoder\")\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", weighted_metrics=[\"mse\"])\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder, name=\"encoder\")\n",
    "\n",
    "print(\"Autoencoder compiled with 2D latent space\")\n",
    "\n",
    "# ====== Helper Functions ======\n",
    "def compute_dbns_ratio(X_latent, labels):\n",
    "    \"\"\"\n",
    "    Compute DBNS (Silhouette-to-Davies-Bouldin) ratio.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_latent : array-like\n",
    "        Data in latent space\n",
    "    labels : array-like\n",
    "        Cluster labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        DBNS ratio (higher is better)\n",
    "    \"\"\"\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return 0.0  # Cannot compute with single cluster\n",
    "    sil = silhouette_score(X_latent, labels)\n",
    "    dbi = davies_bouldin_score(X_latent, labels)\n",
    "    return sil / dbi if dbi > 1e-8 else 0.0\n",
    "\n",
    "def compute_mahalanobis_scores(X_latent, labels, centroids, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute robust Mahalanobis distance to assigned centroid for each point.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_latent : array-like\n",
    "        Data in latent space\n",
    "    labels : array-like\n",
    "        Cluster assignments\n",
    "    centroids : array-like\n",
    "        Cluster centroids\n",
    "    eps : float\n",
    "        Small value for numerical stability\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Mahalanobis distances for each sample\n",
    "    \"\"\"\n",
    "    n, d = X_latent.shape\n",
    "    scores = np.zeros(n, dtype=float)\n",
    "    for c in np.unique(labels):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        Xc = X_latent[idxs]\n",
    "        mu = centroids[c].ravel()\n",
    "        if Xc.shape[0] < 2:\n",
    "            inv_cov = np.linalg.pinv(np.eye(d) * eps)  # Identity matrix for small clusters\n",
    "        else:\n",
    "            cov = np.cov(Xc, rowvar=False)\n",
    "            cov = np.nan_to_num(cov) + eps * np.eye(cov.shape[0])  # Regularize covariance\n",
    "            inv_cov = np.linalg.pinv(cov)\n",
    "        diffs = Xc - mu\n",
    "        scores[idxs] = np.sqrt(np.einsum('ni,ij,nj->n', diffs, inv_cov, diffs))\n",
    "    return scores\n",
    "\n",
    "def adaptive_reweight(scores, tau, theta=0.5):\n",
    "    \"\"\"\n",
    "    Apply adaptive reweighting based on outlier scores.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scores : array-like\n",
    "        Outlier scores (Mahalanobis distances)\n",
    "    tau : float\n",
    "        Threshold for outlier detection\n",
    "    theta : float\n",
    "        Weight for outlier samples\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Sample weights\n",
    "    \"\"\"\n",
    "    w = np.ones(len(scores))\n",
    "    w[scores > tau] = theta  # Reduce weight for outliers\n",
    "    return w\n",
    "\n",
    "def dynamic_tau(scores, lambda_coeff=1.0):\n",
    "    \"\"\"\n",
    "    Compute dynamic threshold τ = μ + λσ for outlier detection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scores : array-like\n",
    "        Outlier scores\n",
    "    lambda_coeff : float\n",
    "        Multiplier for standard deviation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Dynamic threshold value\n",
    "    \"\"\"\n",
    "    mu = float(np.mean(scores))\n",
    "    sd = float(np.std(scores, ddof=1)) if len(scores) > 1 else 0.0\n",
    "    return mu + lambda_coeff * sd\n",
    "\n",
    "def hubness_score(X, k=5):\n",
    "    \"\"\"\n",
    "    Compute hubness score (standard deviation of neighbor counts).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input data\n",
    "    k : int\n",
    "        Number of neighbors\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Hubness score\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "    idx = nbrs.kneighbors(X, return_distance=False)\n",
    "    hubs = np.bincount(idx.flatten(), minlength=len(X))\n",
    "    return float(np.std(hubs))\n",
    "\n",
    "def _auto_perplexity(n):\n",
    "    \"\"\"\n",
    "    Automatically determine perplexity for t-SNE based on dataset size.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Number of samples\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    return int(max(5, min(30, n // 3 - 1))) if n > 20 else 5\n",
    "\n",
    "def plot_tsne_clusters_save(X_latent, labels_km, title_prefix, dataset_base, pdf, random_state=42):\n",
    "    \"\"\"\n",
    "    Create and save t-SNE visualization colored by KMeans clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_latent : array-like\n",
    "        Data in latent space\n",
    "    labels_km : array-like\n",
    "        KMeans cluster labels\n",
    "    title_prefix : str\n",
    "        Plot title prefix\n",
    "    dataset_base : str\n",
    "        Dataset name for file naming\n",
    "    pdf : PdfPages object\n",
    "        PDF object for saving\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    n = X_latent.shape[0]\n",
    "    perplexity = _auto_perplexity(n)\n",
    "    tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\",\n",
    "                perplexity=perplexity, random_state=random_state)\n",
    "    Z = tsne.fit_transform(X_latent)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(Z[:, 0], Z[:, 1], c=labels_km, s=16, alpha=0.9, cmap=\"tab10\")\n",
    "    plt.title(f\"{title_prefix} – t-SNE (KMeans labels)\\nperplexity={perplexity}\")\n",
    "    plt.xticks([]); plt.yticks([]); plt.tight_layout()\n",
    "\n",
    "    base = os.path.join(subdirs[\"tsne\"], f\"{dataset_base}_{title_prefix.replace(' ','_').lower()}_tsne_kmeans\")\n",
    "    plt.savefig(base + \".jpeg\", dpi=200)\n",
    "    plt.savefig(base + \".pdf\")\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "# ====== Sensitivity Analysis Functions ======\n",
    "def run_with_theta(X_scaled, y_true, tau, theta, epochs=30):\n",
    "    \"\"\"\n",
    "    Run a complete clustering iteration with specific tau and theta parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_scaled : array-like\n",
    "        Standardized input features\n",
    "    y_true : array-like\n",
    "        Ground truth labels\n",
    "    tau : float\n",
    "        Outlier threshold\n",
    "    theta : float\n",
    "        Reweighting factor\n",
    "    epochs : int\n",
    "        Training epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (DBNS, ARI, F1, labels) metrics\n",
    "    \"\"\"\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    encoding_dim = 2\n",
    "    \n",
    "    # Create fresh autoencoder for each run\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\", name=\"latent\")(input_layer)\n",
    "    decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
    "    autoencoder = Model(input_layer, decoder, name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", weighted_metrics=[\"mse\"])\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder, name=\"encoder\")\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=8, verbose=0)\n",
    "    \n",
    "    # Get latent representation and cluster\n",
    "    X_latent = encoder_model.predict(X_scaled, verbose=0)\n",
    "    kmeans = KMeans(n_clusters=len(np.unique(y_true)), random_state=42)\n",
    "    labels = kmeans.fit_predict(X_latent)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Compute outlier scores and apply weighting\n",
    "    scores = compute_mahalanobis_scores(X_latent, labels, centroids)\n",
    "    weights = adaptive_reweight(scores, tau, theta=theta)\n",
    "    \n",
    "    # Retrain with sample weights\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=10, batch_size=8, \n",
    "                   sample_weight=weights, verbose=0)\n",
    "    \n",
    "    # Final evaluation\n",
    "    X_latent_final = encoder_model.predict(X_scaled, verbose=0)\n",
    "    kmeans_final = KMeans(n_clusters=len(np.unique(y_true)), random_state=42)\n",
    "    labels_final = kmeans_final.fit_predict(X_latent_final)\n",
    "    \n",
    "    # Compute final metrics\n",
    "    if len(np.unique(labels_final)) > 1:\n",
    "        dbns = compute_dbns_ratio(X_latent_final, labels_final)\n",
    "    else:\n",
    "        dbns = 0.0\n",
    "        \n",
    "    ari = adjusted_rand_score(y_true, labels_final)\n",
    "    f1m = f1_score(y_true, labels_final, average=\"macro\")\n",
    "    \n",
    "    return dbns, ari, f1m, labels_final\n",
    "\n",
    "def plot_lines_with_dataset_name_in_filename(df, x, metrics, title, xlabel, ylabel,\n",
    "                                             pdf_filename_base, dataset_name, output_dir):\n",
    "    \"\"\"\n",
    "    Create and save line plots for sensitivity analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Data to plot\n",
    "    x : str\n",
    "        x-axis column name\n",
    "    metrics : list\n",
    "        y-axis metrics to plot\n",
    "    title : str\n",
    "        Plot title\n",
    "    xlabel : str\n",
    "        x-axis label\n",
    "    ylabel : str\n",
    "        y-axis label\n",
    "    pdf_filename_base : str\n",
    "        Base filename for PDF\n",
    "    dataset_name : str\n",
    "        Dataset name for file naming\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    line_styles = ['-', '--', ':']\n",
    "    for i, m in enumerate(metrics):\n",
    "        plt.plot(df[x], df[m], linestyle=line_styles[i % len(line_styles)],\n",
    "                 marker='o', label=m)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    pdf_filename = os.path.join(\n",
    "        output_dir, f\"{pdf_filename_base.split('.pdf')[0]}_{dataset_name}.pdf\"\n",
    "    )\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close()\n",
    "\n",
    "def sensitivity_dashboard(csv_file,\n",
    "                          tau_grid=[1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                          theta_grid=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "                          epochs=30):\n",
    "    \"\"\"\n",
    "    Run comprehensive sensitivity analysis for τ and θ parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file : str\n",
    "        Path to dataset file\n",
    "    tau_grid : list\n",
    "        Values of τ to test\n",
    "    theta_grid : list\n",
    "        Values of θ to test\n",
    "    epochs : int\n",
    "        Training epochs per configuration\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    data = pd.read_csv(csv_file)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y_true = data.iloc[:, -1].values\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    out_dir = subdirs[\"sensitivity\"]\n",
    "\n",
    "    # τ sweep ---------------------------------------------------------------\n",
    "    print(\"Running τ sensitivity analysis...\")\n",
    "    tau_results = []\n",
    "    for tau in tau_grid:\n",
    "        dbns, ari, f1m, _ = run_with_theta(X_scaled, y_true, tau, theta=1.0, epochs=epochs)\n",
    "        tau_results.append({\"tau\": tau, \"DBNS\": dbns, \"ARI\": ari, \"F1\": f1m})\n",
    "    tau_df = pd.DataFrame(tau_results)\n",
    "    tau_df.to_csv(os.path.join(out_dir, f\"{dataset_name}_tau_sensitivity.csv\"), index=False)\n",
    "\n",
    "    # θ sweep ---------------------------------------------------------------\n",
    "    print(\"Running θ sensitivity analysis...\")\n",
    "    theta_results = []\n",
    "    for th in theta_grid:\n",
    "        dbns, ari, f1m, _ = run_with_theta(X_scaled, y_true, tau=1.5, theta=th, epochs=epochs)\n",
    "        theta_results.append({\"theta\": th, \"DBNS\": dbns, \"ARI\": ari, \"F1\": f1m})\n",
    "    theta_df = pd.DataFrame(theta_results)\n",
    "    theta_df.to_csv(os.path.join(out_dir, f\"{dataset_name}_theta_sensitivity.csv\"), index=False)\n",
    "\n",
    "    # PLOTS -----------------------------------------------------------------\n",
    "    plot_lines_with_dataset_name_in_filename(\n",
    "        tau_df, \"tau\", [\"DBNS\", \"ARI\", \"F1\"],\n",
    "        \"Performance vs τ\", \"τ\", \"Metric value\",\n",
    "        \"performance_vs_tau.pdf\", dataset_name, out_dir\n",
    "    )\n",
    "    plot_lines_with_dataset_name_in_filename(\n",
    "        theta_df, \"theta\", [\"DBNS\", \"ARI\", \"F1\"],\n",
    "        \"Performance vs θ\", \"θ\", \"Metric value\",\n",
    "        \"performance_vs_theta.pdf\", dataset_name, out_dir\n",
    "    )\n",
    "    print(f\"Sensitivity analysis complete for {dataset_name}. Results in {out_dir}\")\n",
    "\n",
    "# ====== λ-SENSITIVITY SWEEP (tau = μ + λσ) ===============================================\n",
    "def lambda_sweep(csv_file,\n",
    "                 lambdas=(0.5, 0.8, 1.0, 1.2, 1.5),\n",
    "                 n_runs=5,\n",
    "                 theta=0.5,\n",
    "                 epochs=30,\n",
    "                 fine_tune_epochs=10,\n",
    "                 choose_by=\"DBNS\"):\n",
    "    \"\"\"\n",
    "    Perform λ-sensitivity analysis for dynamic thresholding.\n",
    "    \n",
    "    For each λ, train an AE, cluster in latent, compute Mahalanobis scores,\n",
    "    set τ = μ + λσ, re-weight (θ below/above τ), fine-tune, re-cluster and record:\n",
    "      - DBNS (Silhouette/DBI)\n",
    "      - ARI\n",
    "      - Outlier% (= fraction scores>τ * 100)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file : str\n",
    "        Path to dataset file\n",
    "    lambdas : tuple\n",
    "        λ values to test\n",
    "    n_runs : int\n",
    "        Number of runs per λ for statistical significance\n",
    "    theta : float\n",
    "        Reweighting factor\n",
    "    epochs : int\n",
    "        Initial training epochs\n",
    "    fine_tune_epochs : int\n",
    "        Fine-tuning epochs with weights\n",
    "    choose_by : str\n",
    "        Metric to optimize (\"DBNS\" or \"ARI\")\n",
    "        \n",
    "    Prints:\n",
    "    -------\n",
    "    [λ-sweep] Suggested λ = ... (S/DB=...±..., ARI=...±..., Outlier=...%±...%)\n",
    "    \"\"\"\n",
    "    ds = os.path.splitext(os.path.basename(csv_file))[-2]\n",
    "    data = pd.read_csv(csv_file)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y_true = data.iloc[:, -1].values\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    out_root = os.path.join(subdirs[\"sensitivity\"], \"lambda_sweep\", ds)\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    def _one_run(lambda_coeff):\n",
    "        \"\"\"Run a single iteration with specific lambda coefficient.\"\"\"\n",
    "        # Fresh AE per run\n",
    "        inp = Input(shape=(X_scaled.shape[1],))\n",
    "        z   = Dense(2, activation=\"relu\")(inp)\n",
    "        out = Dense(X_scaled.shape[1], activation=\"linear\")(z)\n",
    "        ae  = Model(inp, out)\n",
    "        ae.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
    "        enc = Model(inp, z)\n",
    "\n",
    "        ae.fit(X_scaled, X_scaled, epochs=epochs, batch_size=8,\n",
    "               validation_split=0.2, shuffle=True, verbose=0)\n",
    "\n",
    "        Z = enc.predict(X_scaled, verbose=0)\n",
    "        km = KMeans(n_clusters=len(np.unique(y_true)), random_state=rng.randint(0, 10_000)).fit(Z)\n",
    "        labels = km.labels_\n",
    "        centers = km.cluster_centers_\n",
    "\n",
    "        scores = compute_mahalanobis_scores(Z, labels, centers)\n",
    "        tau_l = dynamic_tau(scores, lambda_coeff=lambda_coeff)\n",
    "        w = adaptive_reweight(scores, tau_l, theta=theta)\n",
    "\n",
    "        # brief fine-tune with weights\n",
    "        if fine_tune_epochs > 0:\n",
    "            ae.fit(X_scaled, X_scaled, sample_weight=w, epochs=fine_tune_epochs,\n",
    "                   batch_size=8, validation_split=0.2, shuffle=True, verbose=0)\n",
    "\n",
    "        # final metrics\n",
    "        Z2 = enc.predict(X_scaled, verbose=0)\n",
    "        km2 = KMeans(n_clusters=len(np.unique(y_true)), random_state=rng.randint(0, 10_000)).fit(Z2)\n",
    "        lab2 = km2.labels_\n",
    "\n",
    "        if len(np.unique(lab2)) > 1:\n",
    "            dbns = compute_dbns_ratio(Z2, lab2)\n",
    "        else:\n",
    "            dbns = 0.0\n",
    "        ari = adjusted_rand_score(y_true, lab2)\n",
    "        outlier_rate = float(np.mean(scores > tau_l) * 100.0)  # %\n",
    "        return dbns, ari, outlier_rate\n",
    "\n",
    "    # run sweep\n",
    "    print(f\"Running λ-sweep with {n_runs} runs per λ value...\")\n",
    "    rows = []\n",
    "    for lam in lambdas:\n",
    "        dbns_runs = []\n",
    "        ari_runs = []\n",
    "        out_runs = []\n",
    "        for _ in range(n_runs):\n",
    "            d, a, o = _one_run(lam)\n",
    "            dbns_runs.append(d); ari_runs.append(a); out_runs.append(o)\n",
    "        rows.append({\n",
    "            \"lambda\": lam,\n",
    "            \"DBNS_mean\": np.mean(dbns_runs), \"DBNS_std\": np.std(dbns_runs, ddof=0),\n",
    "            \"ARI_mean\":  np.mean(ari_runs),  \"ARI_std\":  np.std(ari_runs,  ddof=0),\n",
    "            \"Outlier_mean\": np.mean(out_runs), \"Outlier_std\": np.std(out_runs, ddof=0)\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(out_root, f\"{ds}_lambda_sweep.csv\")\n",
    "    res_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # pick best λ based on specified metric\n",
    "    if choose_by.upper() == \"ARI\":\n",
    "        best_idx = res_df[\"ARI_mean\"].values.argmax()\n",
    "    else:\n",
    "        best_idx = res_df[\"DBNS_mean\"].values.argmax()\n",
    "    best = res_df.iloc[best_idx]\n",
    "\n",
    "    # create visualization plots\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.errorbar(res_df[\"lambda\"], res_df[\"DBNS_mean\"], yerr=res_df[\"DBNS_std\"], marker='o', label=\"DBNS\")\n",
    "    plt.xlabel(\"λ\"); plt.ylabel(\"DBNS\"); plt.title(f\"DBNS vs λ — {ds}\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_root, f\"{ds}_lambda_vs_dbns.pdf\")); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.errorbar(res_df[\"lambda\"], res_df[\"ARI_mean\"], yerr=res_df[\"ARI_std\"], marker='o', label=\"ARI\")\n",
    "    plt.xlabel(\"λ\"); plt.ylabel(\"ARI\"); plt.title(f\"ARI vs λ — {ds}\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_root, f\"{ds}_lambda_vs_ari.pdf\")); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.errorbar(res_df[\"lambda\"], res_df[\"Outlier_mean\"], yerr=res_df[\"Outlier_std\"], marker='o', label=\"Outlier %\")\n",
    "    plt.xlabel(\"λ\"); plt.ylabel(\"Outlier (%)\"); plt.title(f\"Outlier% vs λ — {ds}\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_root, f\"{ds}_lambda_vs_outlier.pdf\")); plt.close()\n",
    "\n",
    "    # print summary line\n",
    "    print(f\"[λ-sweep] Suggested λ = {best['lambda']:.2f} \"\n",
    "          f\"(S/DB={best['DBNS_mean']:.3f}±{best['DBNS_std']:.3f}, \"\n",
    "          f\"ARI={best['ARI_mean']:.3f}±{best['ARI_std']:.3f}, \"\n",
    "          f\"Outlier={best['Outlier_mean']:.1f}%±{best['Outlier_std']:.1f}%)\")\n",
    "    print(f\"Saved λ-sweep CSV/plots in: {out_root}\")\n",
    "\n",
    "# ====== Ablation Study Functions ======\n",
    "def run_clustering_variant(X_scaled, y_true, tau, theta, remove_dbns=False, \n",
    "                          remove_reweighting=False, epochs=50):\n",
    "    \"\"\"\n",
    "    Run clustering with specific ablation conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_scaled : array-like\n",
    "        Standardized features\n",
    "    y_true : array-like\n",
    "        Ground truth labels\n",
    "    tau : float\n",
    "        Outlier threshold\n",
    "    theta : float\n",
    "        Reweighting factor\n",
    "    remove_dbns : bool\n",
    "        Whether to remove DBNS from loss\n",
    "    remove_reweighting : bool\n",
    "        Whether to remove reweighting\n",
    "    epochs : int\n",
    "        Training epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Performance metrics for this variant\n",
    "    \"\"\"\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    encoding_dim = 2\n",
    "    \n",
    "    # Create new autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\", name=\"latent\")(input_layer)\n",
    "    decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
    "    autoencoder = Model(input_layer, decoder, name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", weighted_metrics=[\"mse\"])\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder, name=\"encoder\")\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=8, verbose=0)\n",
    "    \n",
    "    # Get latent representation\n",
    "    X_latent = encoder_model.predict(X_scaled, verbose=0)\n",
    "    kmeans = KMeans(n_clusters=len(np.unique(y_true)), random_state=42)\n",
    "    labels = kmeans.fit_predict(X_latent)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = compute_mahalanobis_scores(X_latent, labels, centroids)\n",
    "    \n",
    "    # Apply ablation conditions\n",
    "    if remove_reweighting:\n",
    "        weights = np.ones(len(scores))  # No reweighting\n",
    "    else:\n",
    "        weights = adaptive_reweight(scores, tau, theta=theta)\n",
    "    \n",
    "    # Retrain with weights\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=10, batch_size=8, \n",
    "                   sample_weight=weights, verbose=0)\n",
    "    \n",
    "    # Final evaluation\n",
    "    X_latent_final = encoder_model.predict(X_scaled, verbose=0)\n",
    "    kmeans_final = KMeans(n_clusters=len(np.unique(y_true)), random_state=42)\n",
    "    labels_final = kmeans_final.fit_predict(X_latent_final)\n",
    "    \n",
    "    # Compute metrics\n",
    "    if len(np.unique(labels_final)) > 1:\n",
    "        sil = silhouette_score(X_latent_final, labels_final)\n",
    "        dbi = davies_bouldin_score(X_latent_final, labels_final)\n",
    "        dbns = sil / dbi if dbi > 1e-8 else 0.0\n",
    "    else:\n",
    "        sil = 0.0\n",
    "        dbi = 0.0\n",
    "        dbns = 0.0\n",
    "        \n",
    "    ari = adjusted_rand_score(y_true, labels_final)\n",
    "    f1m = f1_score(y_true, labels_final, average=\"macro\")\n",
    "    \n",
    "    # If DBNS is removed, we don't use it in any loss (still reported as a metric)\n",
    "    hybrid_loss = float(np.mean(scores**2)) - (0.0 if remove_dbns else dbns)\n",
    "    \n",
    "    return {\n",
    "        'DBNS': dbns,\n",
    "        'ARI': ari,\n",
    "        'F1': f1m,\n",
    "        'Silhouette': sil,\n",
    "        'DBI': dbi,\n",
    "        'HybridLoss': hybrid_loss\n",
    "    }\n",
    "\n",
    "def ablation_experiment(csv_file, tau=1.5, theta=0.5, epochs=50):\n",
    "    \"\"\"\n",
    "    Run comprehensive ablation study comparing method variants.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file : str\n",
    "        Path to dataset file\n",
    "    tau : float\n",
    "        Outlier threshold\n",
    "    theta : float\n",
    "        Reweighting factor\n",
    "    epochs : int\n",
    "        Training epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Ablation study results\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    data = pd.read_csv(csv_file)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y_true = data.iloc[:, -1].values\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    out_dir = subdirs[\"ablation\"]\n",
    "    \n",
    "    # 1. Full method (with DBNS and re-weighting)\n",
    "    print(\"Running Full method variant...\")\n",
    "    res_full = run_clustering_variant(X_scaled, y_true, tau, theta,\n",
    "                                      remove_dbns=False, remove_reweighting=False, epochs=epochs)\n",
    "    # 2. Remove DBNS (only S/DB provided, DBNS not used)\n",
    "    print(\"Running No-DBNS variant...\")\n",
    "    res_no_dbns = run_clustering_variant(X_scaled, y_true, tau, theta,\n",
    "                                         remove_dbns=True, remove_reweighting=False, epochs=epochs)\n",
    "    # 3. Remove re-weighting (force theta=1.0)\n",
    "    print(\"Running No-Reweighting variant...\")\n",
    "    res_no_reweight = run_clustering_variant(X_scaled, y_true, tau, 1.0,\n",
    "                                             remove_dbns=False, remove_reweighting=True, epochs=epochs)\n",
    "    \n",
    "    results = pd.DataFrame([\n",
    "        {'Variant': 'Full', **res_full},\n",
    "        {'Variant': 'No DBNS', **res_no_dbns},\n",
    "        {'Variant': 'No Re-weighting', **res_no_reweight}\n",
    "    ])\n",
    "\n",
    "    # Save results\n",
    "    results.to_csv(os.path.join(out_dir, f\"{dataset_name}_ablation_results.csv\"), index=False)\n",
    "    \n",
    "    # Create ablation plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['DBNS', 'ARI', 'F1', 'Silhouette']\n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    full_vals = [results[results['Variant'] == 'Full'][m].values[0] for m in metrics]\n",
    "    no_dbns_vals = [results[results['Variant'] == 'No DBNS'][m].values[0] for m in metrics]\n",
    "    no_reweight_vals = [results[results['Variant'] == 'No Re-weighting'][m].values[0] for m in metrics]\n",
    "    \n",
    "    plt.bar(x_pos - width, full_vals, width, label='Full Method')\n",
    "    plt.bar(x_pos, no_dbns_vals, width, label='No DBNS')\n",
    "    plt.bar(x_pos + width, no_reweight_vals, width, label='No Re-weighting')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Ablation Study - {dataset_name}')\n",
    "    plt.xticks(x_pos, metrics)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(out_dir, f\"{dataset_name}_ablation_study.pdf\"))\n",
    "    plt.savefig(os.path.join(out_dir, f\"{dataset_name}_ablation_study.jpeg\"), dpi=200)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Ablation study complete for {dataset_name}\")\n",
    "    print(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====== Callback to log clustering metrics per epoch ======\n",
    "class ClusteringMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to track clustering metrics during autoencoder training.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_scaled, y_true, encoder_model, true_k, save_prefix):\n",
    "        super().__init__()\n",
    "        self.X = X_scaled\n",
    "        self.y_true = np.asarray(y_true)\n",
    "        self.encoder_model = encoder_model\n",
    "        self.true_k = true_k\n",
    "        self.save_prefix = save_prefix\n",
    "        self.history = {\"epoch\": [], \"ARI\": [], \"F1\": [], \"DBNS\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Compute clustering metrics at the end of each epoch.\"\"\"\n",
    "        X_latent = self.encoder_model.predict(self.X, verbose=0)\n",
    "        kmeans = KMeans(n_clusters=self.true_k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X_latent)\n",
    "        self.history[\"epoch\"].append(epoch + 1)\n",
    "        self.history[\"ARI\"].append(adjusted_rand_score(self.y_true, labels))\n",
    "        self.history[\"F1\"].append(f1_score(self.y_true, labels, average=\"macro\"))\n",
    "        self.history[\"DBNS\"].append(compute_dbns_ratio(X_latent, labels))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Save clustering metrics history at the end of training.\"\"\"\n",
    "        hist_df = pd.DataFrame(self.history)\n",
    "        csv_path = os.path.join(subdirs[\"csv\"], self.save_prefix + \"_cluster_metrics_per_epoch.csv\")\n",
    "        hist_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# ====== Main Training Pipeline ======\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING HYBRID CLUSTERING WITH DBNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize callback and early stopping\n",
    "cm_prefix = dataset_base\n",
    "metrics_cb = ClusteringMetricsCallback(X_scaled, y_true, encoder_model, true_k, cm_prefix)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train autoencoder\n",
    "print(\"Training autoencoder...\")\n",
    "history = autoencoder.fit(\n",
    "    X_scaled, X_scaled,\n",
    "    epochs=max_epochs, batch_size=batch_size,\n",
    "    validation_split=val_split, shuffle=True, verbose=0,\n",
    "    callbacks=[early_stop, metrics_cb]\n",
    ")\n",
    "\n",
    "# ====== Save training history CSV ======\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.to_csv(os.path.join(subdirs[\"csv\"], f\"{dataset_base}_ae_history.csv\"), index=False)\n",
    "\n",
    "# ====== Combined PDF path ======\n",
    "pdf_path = os.path.join(subdirs[\"pdf\"], f\"{dataset_base}_results.pdf\")\n",
    "pdf = PdfPages(pdf_path)\n",
    "\n",
    "# ====== Learning curves (learning/ + combined PDF) ======\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss (MSE)\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss (MSE)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Learning Curves – {dataset_base}\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(os.path.join(subdirs[\"learning\"], f\"{dataset_base}_learning_curves.jpeg\"), dpi=200)\n",
    "plt.savefig(os.path.join(subdirs[\"learning\"], f\"{dataset_base}_learning_curves.pdf\"))\n",
    "pdf.savefig(); plt.close()\n",
    "\n",
    "# ====== Clustering metrics over epochs (metrics/ + combined PDF) ======\n",
    "cm_df = pd.read_csv(os.path.join(subdirs[\"csv\"], cm_prefix + \"_cluster_metrics_per_epoch.csv\"))\n",
    "plt.figure()\n",
    "plt.plot(cm_df[\"epoch\"], cm_df[\"ARI\"], label=\"ARI\")\n",
    "plt.plot(cm_df[\"epoch\"], cm_df[\"F1\"], label=\"F1 (macro)\")\n",
    "plt.plot(cm_df[\"epoch\"], cm_df[\"DBNS\"], label=\"DBNS\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\")\n",
    "plt.title(f\"Clustering Metrics – {dataset_base}\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(os.path.join(subdirs[\"metrics\"], f\"{dataset_base}_cluster_metrics.jpeg\"), dpi=200)\n",
    "plt.savefig(os.path.join(subdirs[\"metrics\"], f\"{dataset_base}_cluster_metrics.pdf\"))\n",
    "pdf.savefig(); plt.close()\n",
    "\n",
    "# ====== Initial latent, clustering, and scores ======\n",
    "print(\"Computing initial clustering metrics...\")\n",
    "X_latent = encoder_model.predict(X_scaled, verbose=0)\n",
    "kmeans = KMeans(n_clusters=true_k, random_state=42)\n",
    "labels = kmeans.fit_predict(X_latent)\n",
    "centroids = kmeans.cluster_centers_\n",
    "scores = compute_mahalanobis_scores(X_latent, labels, centroids)\n",
    "tau = initial_tau\n",
    "\n",
    "# ====== t-SNE BEFORE (tsne/ + combined PDF) ======\n",
    "plot_tsne_clusters_save(X_latent, labels, \"Before DBNS\", dataset_base, pdf)\n",
    "\n",
    "# ====== Baseline metrics BEFORE refinement ======\n",
    "if len(np.unique(labels)) > 1:\n",
    "    silhouette = silhouette_score(X_latent, labels)\n",
    "    dbi = davies_bouldin_score(X_latent, labels)\n",
    "    s_db_ratio = silhouette / dbi if dbi != 0 else 0.0\n",
    "else:\n",
    "    silhouette = 0.0; dbi = 0.0; s_db_ratio = 0.0\n",
    "\n",
    "ari = adjusted_rand_score(y_true, labels)\n",
    "f1 = f1_score(y_true, labels, average='macro')\n",
    "hub_score = hubness_score(X_latent)\n",
    "\n",
    "roc_auc = None\n",
    "if enable_outlier_detection and len(np.unique(y_true)) == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ====== Iterative DBNS refinement ======\n",
    "refine_metrics = []\n",
    "for refine_iter in range(max_iters):\n",
    "    tau = dynamic_tau(scores, lambda_coeff=1.0)\n",
    "    weights = adaptive_reweight(scores, tau, theta=theta)\n",
    "\n",
    "    # (Optional) brief re-train with weights:\n",
    "    # autoencoder.fit(X_scaled, X_scaled, epochs=10, batch_size=batch_size, sample_weight=weights, verbose=0)\n",
    "\n",
    "    X_latent = encoder_model.predict(X_scaled, verbose=0)\n",
    "    kmeans = KMeans(n_clusters=true_k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_latent)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    scores = compute_mahalanobis_scores(X_latent, labels, centroids)\n",
    "\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silscore = silhouette_score(X_latent, labels)\n",
    "        dbi = davies_bouldin_score(X_latent, labels)\n",
    "        dbns = silscore / dbi if dbi > 1e-8 else 0.0\n",
    "    else:\n",
    "        dbns = 0.0\n",
    "\n",
    "    ari_it = adjusted_rand_score(y_true, labels)\n",
    "    f1m_it = f1_score(y_true, labels, average=\"macro\")\n",
    "\n",
    "    dists = np.linalg.norm(X_latent - centroids[labels], axis=1)\n",
    "    recons_loss = np.average(dists**2, weights=weights)\n",
    "    outlier_penalty = float(np.sum(scores[weights < 1.0]))\n",
    "    beta = 1.0; alpha = 0.1\n",
    "    hybrid_loss = recons_loss + alpha * outlier_penalty - beta * dbns\n",
    "\n",
    "    refine_metrics.append({\n",
    "        \"iter\": refine_iter + 1,\n",
    "        \"tau\": float(tau),\n",
    "        \"DBNS\": float(dbns),\n",
    "        \"ARI\": float(ari_it),\n",
    "        \"F1\": float(f1m_it),\n",
    "        \"HybridLoss\": float(hybrid_loss)\n",
    "    })\n",
    "    print(f\"[Refine {refine_iter+1}] tau={tau:.3f}, DBNS={dbns:.3f}, ARI={ari_it:.3f}, F1={f1m_it:.3f}, HybridLoss={hybrid_loss:.3f}\")\n",
    "\n",
    "# Save refinement metrics CSV\n",
    "pd.DataFrame(refine_metrics).to_csv(os.path.join(subdirs[\"csv\"], f\"{dataset_base}_refine_metrics.csv\"), index=False)\n",
    "\n",
    "# ====== Final t-SNE AFTER (tsne/ + combined PDF) ======\n",
    "X_latent_final = encoder_model.predict(X_scaled, verbose=0)\n",
    "kmeans_final = KMeans(n_clusters=true_k, random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_latent_final)\n",
    "plot_tsne_clusters_save(X_latent_final, labels_final, \"After DBNS\", dataset_base, pdf)\n",
    "\n",
    "# ====== Outlier Score Distribution (outlier/ + combined PDF) ======\n",
    "plt.figure()\n",
    "plt.hist(scores, bins=40)\n",
    "plt.xlabel(\"Mahalanobis distance (anomaly score)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Outlier Score Histogram – {dataset_base}\")\n",
    "plt.tight_layout()\n",
    "base = os.path.join(subdirs[\"outlier\"], f\"{dataset_base}_outlier_scores_hist\")\n",
    "plt.savefig(base + \".jpeg\", dpi=200); plt.savefig(base + \".pdf\"); pdf.savefig(); plt.close()\n",
    "\n",
    "score_by_cluster = [scores[labels_final == c] for c in np.unique(labels_final)]\n",
    "plt.figure()\n",
    "plt.boxplot(score_by_cluster, showfliers=True)\n",
    "plt.xlabel(\"Cluster index\"); plt.ylabel(\"Mahalanobis distance (anomaly score)\")\n",
    "plt.title(f\"Outlier Scores by Cluster – {dataset_base}\")\n",
    "plt.tight_layout()\n",
    "base = os.path.join(subdirs[\"outlier\"], f\"{dataset_base}_outlier_scores_boxplot\")\n",
    "plt.savefig(base + \".jpeg\", dpi=200); plt.savefig(base + \".pdf\"); pdf.savefig(); plt.close()\n",
    "\n",
    "# ====== Run Sensitivity Analyses ======\n",
    "sensitivity_dashboard(file_path)                       # existing τ/θ sweeps\n",
    "lambda_sweep(file_path, lambdas=(0.5,0.8,1.0,1.2,1.5), # NEW λ-sweep\n",
    "             n_runs=5, theta=theta, epochs=30, fine_tune_epochs=10, choose_by=\"DBNS\")\n",
    "\n",
    "# ====== Run Ablation Study ======\n",
    "ablation_experiment(file_path, tau=initial_tau, theta=theta, epochs=30)\n",
    "\n",
    "# ====== Runtime log & console summary ======\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time, 2)\n",
    "timestamp = time.strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "with open(os.path.join(subdirs[\"csv\"], \"execution_log.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{timestamp} Executed clustering on {dataset_name} in {elapsed_time} seconds.\\n\")\n",
    "\n",
    "# ====== Display final metrics ======\n",
    "print(f\"\\nDataset: {dataset_name}\")\n",
    "print(f\"Silhouette: {silhouette:.3f}  DBI: {dbi:.3f}  DBNS: {s_db_ratio:.3f}\")\n",
    "print(f\"ARI: {ari:.3f}  F1: {f1:.3f}  Hubness: {hub_score:.3f}\")\n",
    "if enable_outlier_detection and roc_auc is not None:\n",
    "    print(f\"AUC (Outlier): {roc_auc:.3f}\")\n",
    "elif enable_outlier_detection:\n",
    "    print(\"AUC (Outlier): skipped; requires binary ground truth\")\n",
    "\n",
    "# ====== Close combined PDF ======\n",
    "pdf.close()\n",
    "print(f\"\\nAll results saved in: {plots_dir}\")\n",
    "print(f\"Combined PDF report: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7ecb-3c8f-43e8-a4c9-5deaa46c49de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
